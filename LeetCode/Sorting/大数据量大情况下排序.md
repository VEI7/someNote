## 特别大的数据量，实现查找，排序

###1）、位图法

位图法是我在编程珠玑上看到的一种比较新颖的方法，思路比较巧妙效率也很高。

####使用场景举例：对2G的数据量进行排序，这是基本要求。

数据：1、每个数据不大于8亿；2、数据类型位int；3、每个数据最多重复一次。

内存：最多用200M的内存进行操作。

首先对占用的内存进行判断，每个数据不大于8亿，那么8亿是一个什么概念呢。

**

1 byte = 8 bit（位）

1024 byte = 8*1024 bit = 1k

1024 k = 8*1024*1024 bit = 1M = 8388608 bit

**

也就是1M=8388608位

而位图法的基本思想就是利用一位代表一个数字，例如3位上为1,则说明3在数据中出现过，若为0，则说明3在数据中没有出现过。所以当题目中出现每个数据最多重复一次这个条件时，我们可以考虑使用位图法来进行大数据排序。

那么假如使用位图法来进行这题的排序，内存占用多少呢。由题目知道每个数据不大于8亿，那么我们就需要8亿位，占用800000000/8388608=95M的空间，满足最多使用200M内存进行操作的条件，这也是这题能够使用位图法来解决的一个基础。

```c++
#define SHIFT 5
#define MASK 0x1F

void setBit(int* bit, int val)
{
    bit[val>>SHIFT] |= (1<<(val&MASK));
}

bool getBit(int* bit, int val)
{
    return bit[val>>SHIFT] & (1<<(val&MASK));
}

vector<int> getCommonfriends1(vector<vector<int>>& friends)
{
    vector<int> res;
    unsigned int MAX = (unsigned int)(1<<31);
    unsigned int SIZE = MAX>>5;
    int* bitmap = new int[SIZE+1];
    for(int i=0; i<friends.size();i++)
    {
        memset(bitmap, 0, SIZE+1);
        for(int f : friends[i])
            setBit(bitmap, f);
        for(int j=i+1; j<friends.size(); j++)
        {
            int count=0;
            for(int f : friends[j])
                if(getBit(bitmap, f)) count++;
            res.push_back(count);
        }
    }
    return res;
}

int main() {
    vector<vector<int>> f{{1,2},{0,3,2},{0,1},{1}};
    vector<int> res = getCommonfriends1(f);
    for(auto r : res)
        cout<<r<<endl;
}
```





###2）、堆排序法

堆排序是4种平均时间复杂度为nlogn的排序方法之一，其优点在于当求M个数中的前n个最大数，和最小数的时候性能极好。所以当从海量数据中要找出前m个最大值或最小值，而对其他值没有要求时，使用堆排序法效果很好。

####使用场景：从1亿个整数里找出100个最大的数

步骤：

（1）读取前100个数字，建立最大值堆。（这里采用堆排序将空间复杂度讲得很低，要排序1亿个数，但一次性只需读取100个数字，或者设置其他基数，不需要1次性读完所有数据，降低对内存要求）

（2）依次读取余下的数，与最大值堆作比较，维持最大值堆。可以每次读取的数量为一个磁盘页面，将每个页面的数据依次进堆比较，这样节省IO时间。

（3）将堆进行排序，即可得到100个有序最大值。

堆排序是一种常见的算法，但了解其的使用场景能够帮助我们更好的理解它。

3）、较为通用的分治策略

分治策略师对常见复杂问题的一种万能的解决方法，虽然很多情况下，分治策略的解法都不是最优解，但是其通用性很强。分治法的核心就是将一个复杂的问题通过分解抽象成若干个简单的问题。

####应用场景：10G的数据，在2G内存的单台机器上排序的算法

我的想法，这个场景既没有介绍数据是否有重复，也没有给出数据的范围，也不是求最大的个数。而通过分治虽然可能需要的io次数很多，但是对解决这个问题还是具有一定的可行性的。

步骤：

（1）从大数据中抽取样本，将需要排序的数据切分为多个样本数大致相等的区间，例如：1-100，101-300…

（2）将大数据文件切分为多个小数据文件，这里要考虑IO次数和硬件资源问题，例如可将小数据文件数设定为1G（要预留内存给执行时的程序使用）

（3）使用最优的算法对小数据文件的数据进行排序，将排序结果按照步骤1划分的区间进行存储

（4）对各个数据区间内的排序结果文件进行处理，最终每个区间得到一个排序结果的文件

（5）将各个区间的排序结果合并。通过分治将大数据变成小数据进行处理，再合并。



### 题目：在一个文件中有 10G 个整数，乱序排列，要求找出中位数。内存限制为 2G。只写出思路即可（内存限制为 2G的意思就是，可以使用2G的空间来运行程序，而不考虑这台机器上的其他软件的占用内存）。

关于中位数：数据排序后，位置在最中间的数值。即将数据分成两部分，一部分大于该数值，一部分小于该数值。中位数的位置：当样本数为奇数时，中位数=(N+1)/2 ; 当样本数为偶数时，中位数为N/2与1+N/2的均值（那么10G个数的中位数，就第5G大的数与第5G+1大的数的均值了）。

分析： 既然要找中位数，很简单就是排序的想法。那么基于字节的桶排序是一个可行的方法 

思想：将整形的每1byte作为一个关键字，也就是说一个整形可以拆成4个keys，而且最高位的keys越大，整数越大。如果高位keys相同，则比较次高位的keys。整个比较过程类似于字符串的字典序。

第一步:把10G整数每2G读入一次内存，然后一次遍历这536,870,912个数据。每个数据用位运算">>"取出最高8位(31-24)。这8bits(0-255)最多表示255个桶，那么可以根据8bit的值来确定丢入第几个桶。最后把每个桶写入一个磁盘文件中，同时在内存中统计每个桶内数据的数量，自然这个数量只需要255个整形空间即可。

代价：

10G数据依次读入内存的IO代价(这个是无法避免的，CPU不能直接在磁盘上运算)。
在内存中遍历536,870,912个数据，这是一个O(n)的线性时间复杂度。
把255个桶写会到255个磁盘文件空间中，这个代价是额外的，也就是多付出一倍的10G数据转移的时间。

第二步：根据内存中255个桶内的数量，计算中位数在第几个桶中。很显然，2,684,354,560个数中位数是第1,342,177,280个。假设前127个桶的数据量相加，发现少于1,342,177,280，把第128个桶数据量加上，大于1,342,177,280。说明，中位数必在磁盘的第128个桶中。而且在这个桶的第1,342,177,280-N(0-127)个数位上。N(0-127)表示前127个桶的数据量之和。然后把第128个文件中的整数读入内存。(平均而言，每个文件的大小估计在10G/128=80M左右，当然也不一定，但是超过2G的可能性很小)。

代价：

循环计算255个桶中的数据量累加，需要O(M)的代价，其中m<255。
读入一个大概80M左右文件大小的IO代价。

注意，变态的情况下，这个需要读入的第128号文件仍然大于2G，那么整个读入仍然可以按照第一步分批来进行读取。

第三步：继续以内存中的整数的次高8bit进行桶排序(23-16)。过程和第一步相同，也是255个桶。


第四步：一直下去，直到最低字节(7-0bit)的桶排序结束。我相信这个时候完全可以在内存中使用一次快排就可以了。

 

整个过程的时间复杂度在O(n)的线性级别上(没有任何循环嵌套)。但主要时间消耗在第一步的第二次内存-磁盘数据交换上，即10G数据分255个文件写回磁盘上。一般而言，如果第二步过后，内存可以容纳下存在中位数的某一个文件的话，直接快排就可以了。

### 10G的文件，2G内存，统计出现频率最高的数字

读入部分文件，对数值模10.取值相同放入一个文件。然后处理10个文件。统计出现次数最多的。

 我认为上面这种方案可以解决一种情况。就是文件数值不重复。或者重复较少的情况

假设一种极端情况。文件内容全部取模值全部相同。或者超过2G就不在适用上面的方法了。而更适合将相同的数字放入同一个文件。
